# The Shape of Development {#shape}

However, we define the underlying metric of time to structure our longitudinal model, one of the key substantive questions often underlying work in developmental science is to characterize the course that a given construct takes over time. Here we will highlight many of the different developmental trajectories that we can fit to our data, starting with relatively simple polynomial shapes and working our way up to modeling fully nonlinear trends. In addition to the `feedback.learning` data we have used thus far, we will also use data drawn from the `external-math.csv` and `adversity.csv` files. The `external.math` data contains up to $5$ repeated observations from $405$ children aged $6$ to $14$, measured once every $2$ years. Here we will focus on measures of externalizing behavior and math proficiency. The `adversity` data contains fractional anisotropy (FA) measures from $398$ children measured up to $4$ times across ages $4$ to $11$. We previously used a subset of this data in the [Time](#time) chapter, but here we will utilize the entire sample.

```{r read data shape, message = FALSE, warning = FALSE, error = FALSE}
external.math <- read.csv("data/external-math.csv")

adversity <- read.csv("data/adversity.csv")

feedback.learning <- read.csv("data/feedback-learning.csv") %>% 
  select(id, age, modularity, learning.rate)
```

## Polynomial Trajectories

Like we discussed in the main text, polynomial trajectories are far and away the most common trajectories modeled with longitudinal data. They require relatively few unique timepoints, are straightforward to model, and offer easily-interpretable parameter estimates.

### Intercept-Only Model

We can first consider the simplest polynomial model, one without even a slope. The intercept-only model simply models person-specific differences in average level across time. We can start here with the LCM, which makes the various specifications easiest to see, but we will also build syntax for models in the other frameworks.

```{r polynomials 01, message = FALSE, warning = FALSE, error = FALSE}
int.lcm <- "int =~ 1*ext6 + 1*ext8 + 1*ext10 + 1*ext12 + 1*ext14"

int.lcm.fit <- growth(int.lcm, 
                      data = external.math,
                      estimator = "ML",
                      missing = "FIML")

summary(int.lcm.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = FALSE, rsquare = FALSE)
```

We can see that there is significant variance in the intercept factor, suggesting meaningful person-to-person variability in level of externalizing behavior during late childhood and early adolescence. While this might seem a somewhat silly model to fit to these data, this is one half of a random-intercept cross-lag panel model and might be appropriate if we do not expect systematic change over time. However, these intercept-only models are admittedly more plausible for intensive longitudinal data. The MLM specification for this model can be seen below. We will first transform the data into long format before fitting the model.

```{r polynomials 02, message = FALSE, warning = FALSE, error = FALSE}
external.math.long <- external.math %>% 
  pivot_longer(cols = starts_with(c("ext", "math")), 
                      names_to = c(".value", "age"), 
                      names_pattern = "(ext|math)(.+)") %>%
  mutate(age = as.numeric(age))

int.mlm <- lmer(ext ~ 1 + (1 | id),
                na.action = na.omit,
                REML = TRUE,
                data = external.math.long)

summary(int.mlm, correlation = FALSE)
```

Note that this is just a random-effects ANOVA model with no predictors.

### Linear Model

We will move quickly through the linear polynomial models because we have covered them extensively thus far. Below is the syntax for the linear LCM. Remember that assessments are biannual so factor loadings should increase by two for each wave.

```{r polynomials 03, message = FALSE, warning = FALSE, error = FALSE}
lin.lcm <- "int =~ 1*ext6 + 1*ext8 + 1*ext10 + 1*ext12 + 1*ext14
            slp =~ 0*ext6 + 2*ext8 + 4*ext10 + 6*ext12 + 8*ext14"

lin.lcm.fit <- growth(lin.lcm, 
                      data = external.math,
                      estimator = "ML",
                      missing = "FIML")

summary(lin.lcm.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = FALSE, rsquare = FALSE)
```

While we have ignored model fit for most models, one nice thing about many of these models, is that they are nested and allow for formal model comparison with likelihood ratio tests, similar to those we saw with hetero- vs. homoscedastic residuals. For instance, we can compare the intercept-only with a linear model.

```{r polynomials 04, message = FALSE, warning = FALSE, error = FALSE}
lavTestLRT(int.lcm.fit, lin.lcm.fit)
```
Remember that we compare whether the more constrained model (here the intercept-only) induces a significant *decrease* in model fit. Here this is true, suggesting that we should retain the linear model over the intercept-only model. If we take a peak at the model fit, this is because the linear model fits the data reasonably well, while the intercept-only model is quite poor in terms of fit.

```{r polynomials 05, message = FALSE, warning = FALSE, error = FALSE}
summary(int.lcm.fit, fit.measures = TRUE, estimates = FALSE, 
        standardize = FALSE, rsquare = FALSE)

summary(lin.lcm.fit, fit.measures = TRUE, estimates = FALSE, 
        standardize = FALSE, rsquare = FALSE)
```

To fit the corresponding MLM, we first need to generate our linear predictor as an observed variable in our data frame (and will need to do so each time we increase the order of the model). Here we will generate the linear predictor by simply subtracting $6$ from the `age` variable.

```{r polynomials 06, message = FALSE, warning = FALSE, error = FALSE}
external.math.long$age <- external.math.long$age - min(external.math.long$age)

lin.mlm <- lmer(ext ~ 1 + age + (1 + age | id),
                na.action = na.omit,
                REML = TRUE,
                data = external.math.long,
                control = lmerControl(optimizer = "bobyqa",
                                      optCtrl = list(maxfun = 2e5)))

summary(lin.mlm, correlation = FALSE)
```

And the model comparison reveals the same preference for the linear model. Note that the model is re-estimated with ML [FIML] because the two models contain different fixed effects. REML models can be compared when the models differ only in the variance structure. Fortunately this will be done automatically so we don't have to "manually" re-estimate the models.

```{r polynomials 07, message = FALSE, warning = FALSE, error = FALSE}
anova(int.mlm, lin.mlm)
```

Finally, we can see a version of the linear LCSM for the `external.math` data below. Note again that biannual observations mean that we need to set the factor loadings for the slope factor to $2$ instead of $1$ to indicate the spacing appropriately.

```{r polynomials 08, message = FALSE, warning = FALSE, error = FALSE}
lin.lcsm <- "
            # Define Phantom Variables (p = phantom)
            pext6 =~ 1*ext6; ext6 ~ 0; ext6 ~~ ext6; pext6 ~~ 0*pext6
            pext8 =~ 1*ext8; ext8 ~ 0; ext8 ~~ ext8; pext8 ~~ 0*pext8
            pext10 =~ 1*ext10; ext10 ~ 0; ext10 ~~ ext10; pext10 ~~ 0*pext10
            pext12 =~ 1*ext12; ext12 ~ 0; ext12 ~~ ext12; pext12 ~~ 0*pext12
            pext14 =~ 1*ext14; ext14 ~ 0; ext14 ~~ ext14; pext14 ~~ 0*pext14
        
            # Regressions Between Adjacent Observations
            pext8 ~ 1*pext6
            pext10 ~ 1*pext8
            pext12 ~ 1*pext10
            pext14 ~ 1*pext12
        
            # Define Change Latent Variables (delta)
            delta21 =~ 1*pext8;  delta21 ~~ 0*delta21
            delta32 =~ 1*pext10; delta32 ~~ 0*delta32
            delta43 =~ 1*pext12; delta43 ~~ 0*delta43
            delta54 =~ 1*pext14; delta54 ~~ 0*delta54
        
            # Define Intercept and Slope
            int =~ 1*pext6
            slp =~ 2*delta21 + 2*delta32 + 2*delta43 + 2*delta54
        
            int ~ 1
            slp ~ 1
            
            int ~~ slp
            slp ~~ slp
"

lin.lcsm.fit <- sem(lin.lcsm, 
                    data = external.math, 
                    estimator = "ML",
                    missing = "FIML")

summary(lin.lcsm.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = FALSE, rsquare = FALSE)
```

### Quadratic Model 

Next, we can add an additional factor to capture quadratic curvature in our data. Below is the LCM syntax for this model. Note that this is an extremely straightforward expansion of the syntax we have seen thus far. While this won't be the case here, sometimes we need to worry about numerically large factor loadings causing some estimation issues in practice (nothing theoretical is wrong with large factor loadings). In those instances, we could divide our factor loadings by some constant to control those values from getting to large (although this will change the interpretation of a per-unit change).

```{r polynomials 09, message = FALSE, warning = FALSE, error = FALSE}
quad.lcm <- "int  =~ 1*ext6 + 1*ext8 + 1*ext10 + 1*ext12 + 1*ext14
             slp  =~ 0*ext6 + 2*ext8 + 4*ext10 + 6*ext12 + 8*ext14
             quad =~ 0*ext6 + 4*ext8 + 16*ext10 + 36*ext12 + 64*ext14"

quad.lcm.fit <- growth(quad.lcm, 
                       data = external.math,
                       estimator = "ML",
                       missing = "FIML")

summary(quad.lcm.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = TRUE, rsquare = FALSE)

lavTestLRT(lin.lcm.fit, quad.lcm.fit)
```

The MLM syntax is similarly straightforward. To add a powered term, we can use the `I()` function, or the `poly()` function if we wished to use orthogonal polynomials.

```{r polynomials 10, message = FALSE, warning = FALSE, error = FALSE}
quad.mlm <- lmer(ext ~ 1 + age + I(age^2) + (1 + age + I(age^2) | id),
                 na.action = na.omit,
                 REML = TRUE,
                 data = external.math.long,
                 control = lmerControl(optimizer = "bobyqa",
                                       optCtrl = list(maxfun = 2e5)))

summary(quad.mlm, correlation = FALSE)
```

While these models converge without too much issue here, note the strong correlation between the linear and quadratic terms, suggesting that the quadratic term is largely redundant. This is often the case with low numbers of repeated measures. We can technically fit some non-linearities, but they may not be particularly well-specified.

The LCSM syntax requires a bit more explanation. The quadratic model is shown below.

```{r polynomials 11, message = FALSE, warning = FALSE, error = FALSE}
quad.lcsm <- "
             # Define Phantom Variables (p = phantom)
             pext6 =~ 1*ext6; ext6 ~ 0; ext6 ~~ ext6; pext6 ~~ 0*pext6
             pext8 =~ 1*ext8; ext8 ~ 0; ext8 ~~ ext8; pext8 ~~ 0*pext8
             pext10 =~ 1*ext10; ext10 ~ 0; ext10 ~~ ext10; pext10 ~~ 0*pext10
             pext12 =~ 1*ext12; ext12 ~ 0; ext12 ~~ ext12; pext12 ~~ 0*pext12
             pext14 =~ 1*ext14; ext14 ~ 0; ext14 ~~ ext14; pext14 ~~ 0*pext14
        
             # Regressions Between Adjacent Observations
             pext8 ~ 1*pext6
             pext10 ~ 1*pext8
             pext12 ~ 1*pext10
             pext14 ~ 1*pext12
        
             # Define Change Latent Variables (delta)
             delta21 =~ 1*pext8;  delta21 ~~ 0*delta21
             delta32 =~ 1*pext10; delta32 ~~ 0*delta32
             delta43 =~ 1*pext12; delta43 ~~ 0*delta43
             delta54 =~ 1*pext14; delta54 ~~ 0*delta54
        
             # Define Intercept and Slope
             int  =~ 1*pext6
             slp  =~ 2*delta21 + 2*delta32 + 2*delta43 + 2*delta54
             quad =~ 4*delta21 + 12*delta32 + 20*delta43 + 28*delta54 
        
             int  ~ 1
             slp  ~ 1
             quad ~ 1
            
             int ~~ slp
             int ~~ quad
             slp ~~ slp
             slp ~~ quad
             quad ~~ quad
"
```

We talked in the [Canonical](#canon) chapter about how the loadings for the linear factor were all $1$, and this could be thought of as summing across the difference factors. Another way to think of this specification is that the factor loadings for the LCSM are the *differences* between successive loadings for the LCM. In the standard linear case, these are all $1$s to indicate a constant effect across units of time, whereas in our example in this chapter, they are all differences of $2$ to reflect biannual observations. The same principle can be applied to the loadings for higher-order factors in the LCSM. For a quadratic factor, the LCM loadings are [$0$, $4$, $16$, $36$, $64$], and therefore the LCSM loadings should be [($4 - 0$), ($16 - 4$), ($36 - 16$), ($64 - 36$)] = [$4$, $12$, $20$, $28$]. As a sanity check, we can fit this model and the parameter estimates should match the LCM results exactly.

```{r polynomials 12, message = FALSE, warning = FALSE, error = FALSE}
quad.lcsm.fit <- sem(quad.lcsm,
                     data = external.math, 
                     estimator = "ML",
                     missing = "FIML")

summary(quad.lcsm.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = TRUE, rsquare = FALSE)

lavTestLRT(lin.lcsm.fit, quad.lcsm.fit)
```

### Inverse Model

The final polynomial model we will consider is the inverse model. Unlike the quadratic curvature which reverses, the inverse curve approaches a plateau asymptotically. We can do a quick algebraic transformation to make the factor loadings tractable by inverting the original (i.e., not centered) values and subtracting them from $1$. So for linear loadings [$1$, $3$, $5$, $7$, $9$], we would have inverse factor loadings [$1 - (1/1)$, $1 - (1/3)$, $1 - (1/5)$, $1 - (1/7)$, $1 - (1/9)$] or [$0$, $2/3$, $4/5$, $6/7$, $8/9$]. Inverting the original loadings avoids trying to take the reciprocal of 0 (which results in 6 more weeks of COVID variants) and subtracting from one specifies an upper rather than lower bound effect (this won't change the nature of the effect, it just makes the sign easier to interpret).

```{r polynomials 13, message = FALSE, warning = FALSE, error = FALSE}
inv.lcm <- "int =~ 1*ext6 + 1*ext8 + 1*ext10 + 1*ext12 + 1*ext14
            slp =~ 0*ext6 + 2*ext8 + 4*ext10 + 6*ext12 + 8*ext14
            inv =~ 0*ext6 + (2/3)*ext8 + (4/5)*ext10 + (6/7)*ext12 + (7/8)*ext14"

inv.lcm.fit <- growth(inv.lcm, 
                      data = external.math,
                      estimator = "ML",
                      missing = "FIML")

summary(inv.lcm.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = TRUE, rsquare = FALSE)

lavTestLRT(lin.lcm.fit, inv.lcm.fit)
```

Note that we are comparing the inverse and linear models, not the inverse and quadratic. This is because while the linear model is nested within both quadratic and inverse models, the two are not nested with respect to one another. However, we might graphically examine the trends implied by the model for a moment.

```{r polynomials 14, message = FALSE, warning = FALSE, error = FALSE, echo = FALSE}
ggplot(data.frame(id=quad.lcm.fit@Data@case.idx[[1]], 
                           lavPredict(quad.lcm.fit,type="ov")) %>% 
                  pivot_longer(cols = starts_with("ext"), 
                               names_to = c(".value", "age"), 
                               names_pattern = "(ext)(.+)") %>%
                  dplyr::mutate(age = as.numeric(age)), 
                aes(x = age, 
                    y = ext, 
                    group = id, 
                    color = factor(id))) +
  geom_line() + 
  labs(title = "Quadratic LCM Trajectories",
       x = "Age",
       y = "Predicted Externalizing Behavior") +
  theme(legend.position = "none") 

ggplot(data.frame(id=inv.lcm.fit@Data@case.idx[[1]], 
                           lavPredict(inv.lcm.fit,type="ov")) %>% 
                  pivot_longer(cols = starts_with("ext"), 
                               names_to = c(".value", "age"), 
                               names_pattern = "(ext)(.+)") %>%
                  dplyr::mutate(age = as.numeric(age)), 
                aes(x = age, 
                    y = ext, 
                    group = id, 
                    color = factor(id))) +
  geom_line() + 
  labs(title = "Inverse LCM Trajectories",
       x = "Age",
       y = "Predicted Externalizing Behavior") +
  theme(legend.position = "none") 
```

What should be visually apparent is that we get quite a few flips in the direction of curvature in the inverse compared to the quadratic model. Indeed the quadratic effect is negative ($-0.015$) and the inverse effect is positive ($0.513$). This sensitivity is likely another indication that curvature is really over-fitting noise in these data rather than reflecting some true non-linearity. Below are how to achieve this model with the MLM:

```{r polynomials 15, message = FALSE, warning = FALSE, error = FALSE}
external.math.long$age_inv <- 1 - (external.math.long$age + 1)^(-1)

inv.mlm <- lmer(ext ~ 1 + age + age_inv + (1 + age + age_inv | id),
                na.action = na.omit,
                REML = TRUE,
                data = external.math.long,
                control = lmerControl(optimizer = "bobyqa",
                                      optCtrl = list(maxfun = 2e5)))

summary(inv.mlm, correlation = FALSE)

anova(lin.mlm, inv.mlm)
```

and LCSM (note that the subtraction method gets a little messy for the slope loadings):

```{r polynomials 16, message = FALSE, warning = FALSE, error = FALSE}
inv.lcsm <- "
            # Define Phantom Variables (p = phantom)
            pext6 =~ 1*ext6; ext6 ~ 0; ext6 ~~ ext6; pext6 ~~ 0*pext6
            pext8 =~ 1*ext8; ext8 ~ 0; ext8 ~~ ext8; pext8 ~~ 0*pext8
            pext10 =~ 1*ext10; ext10 ~ 0; ext10 ~~ ext10; pext10 ~~ 0*pext10
            pext12 =~ 1*ext12; ext12 ~ 0; ext12 ~~ ext12; pext12 ~~ 0*pext12
            pext14 =~ 1*ext14; ext14 ~ 0; ext14 ~~ ext14; pext14 ~~ 0*pext14
        
            # Regressions Between Adjacent Observations
            pext8 ~ 1*pext6
            pext10 ~ 1*pext8
            pext12 ~ 1*pext10
            pext14 ~ 1*pext12
        
            # Define Change Latent Variables (delta)
            delta21 =~ 1*pext8;  delta21 ~~ 0*delta21
            delta32 =~ 1*pext10; delta32 ~~ 0*delta32
            delta43 =~ 1*pext12; delta43 ~~ 0*delta43
            delta54 =~ 1*pext14; delta54 ~~ 0*delta54
        
            # Define Intercept and Slope
            int =~ 1*pext6
            slp =~ 2*delta21 + 2*delta32 + 2*delta43 + 2*delta54
            inv =~ (2/3)*delta21 + (2/15)*delta32 + (2/35)*delta43 + (1/56)*delta54 
        
            int ~ 1
            slp ~ 1
            inv ~ 1
            
            int ~~ slp
            int ~~ inv
            slp ~~ slp
            slp ~~ inv
            inv ~~ inv
"

inv.lcsm.fit <- sem(inv.lcsm,
                    data = external.math, 
                    estimator = "ML",
                    missing = "FIML")

summary(inv.lcsm.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = TRUE, rsquare = FALSE)

lavTestLRT(lin.lcsm.fit, inv.lcsm.fit)
```

## Piecewise Trajectories

If we do not think that a single polynomial function can sufficiently capture a complex trajectory, we might consider bolting two (or more) polynomial functions together using a piecewise approach. Here we will use the `adversity` data which covers $8$ years of childhood (ages $4-11$). The simplest piecewise trajectory can be constructed two distinct linear pieces joined at a knot point. We need at least 3 time points to specify a line but the pieces can share a time point at the knot point. This means we need a minimum of $5$ time points in order to fit even the simplest piecewise model. Note that with this minimum, the knot point is constrained to be at the middle time point, and the knot can never be placed at the first or last two time points because of the 3 time point requirement to estimate the linear slope. Note that as we discussed before, these time point requirements can be accomodated at the group level, and no one individual need be observed $5$ or more times. Indeed this is the case here, where no individual is measured more than $4$ times.

There are two general approaches for specifying piecewise models. The first, and more common, approach is the two-rate specification, where each effect can be interpreted in isolation like a regular linear model. We  specify the two-rate LCM using the syntax below. Note that we code the factor loadings in such a way that the intercept is at the knot point (age 8).

```{r piecewise 01, message = FALSE, warning = FALSE, error = FALSE}
two.rate <- "int  =~ 1*fmin4 + 1*fmin5 + 1*fmin6 + 1*fmin7 + 
                     1*fmin8 + 1*fmin9 + 1*fmin10 + 1*fmin11
             slp1 =~ -3*fmin4 + -2*fmin5 + -1*fmin6 + 0*fmin7 + 
                     0*fmin8 + 0*fmin9 + 0*fmin10 + 0*fmin11
             slp2 =~ 0*fmin4 + 0*fmin5 + 0*fmin6 + 0*fmin7 + 
                     1*fmin8 + 2*fmin9 + 3*fmin10 + 4*fmin11
"

two.rate.fit <- growth(two.rate,
                       data = adversity,
                       estimator = "ML",
                       missing = "FIML")

summary(two.rate.fit, fit.measures = FALSE, estimates = TRUE,
        standardize = TRUE, rsquare = FALSE)

ggplot(data.frame(id=two.rate.fit@Data@case.idx[[1]], 
                           lavPredict(two.rate.fit,type="ov")) %>% 
                  pivot_longer(cols = starts_with("fmin"), 
                               names_to = c(".value", "age"), 
                               names_pattern = "(fmin)(.+)") %>%
                  dplyr::mutate(age = as.numeric(age)), 
                aes(x = age, 
                    y = fmin, 
                    group = id, 
                    color = factor(id))) +
  geom_line() + 
  labs(title = "2-Rate Piecewise LCM Trajectories",
       x = "Age",
       y = "Predicted Forceps Minor Microstructure") +
  theme(legend.position = "none") 
```

```{r piecewise 02, message = FALSE, warning = FALSE, error = FALSE}
add.rate <- "int  =~ 1*fmin4 + 1*fmin5 + 1*fmin6 + 1*fmin7 + 
                     1*fmin8 + 1*fmin9 + 1*fmin10 + 1*fmin11
             slp1 =~ -3*fmin4 + -2*fmin5 + -1*fmin6 + 0*fmin7 + 
                     1*fmin8 + 2*fmin9 + 3*fmin10 + 4*fmin11
             slp2 =~ 0*fmin4 + 0*fmin5 + 0*fmin6 + 0*fmin7 + 
                     1*fmin8 + 2*fmin9 + 3*fmin10 + 4*fmin11
"

add.rate.fit <- growth(add.rate,
                       data = adversity,
                       estimator = "ML",
                       missing = "FIML")

summary(add.rate.fit, fit.measures = FALSE, estimates = TRUE,
        standardize = TRUE, rsquare = FALSE)
```

```{r piecewise 03, message = FALSE, warning = FALSE, error = FALSE}
trials <- read.csv("data/trials.csv")

quad.rate <- "int =~ 1*trial.1 + 1*trial.2 + 1*trial.3 + 1*trial.4 + 
                     1*trial.5 + 1*trial.6 + 1*trial.7
             slp1 =~ -3*trial.1 + -2*trial.2 + -1*trial.3 + 0*trial.4 + 
                     0*trial.5 + 0*trial.6 + 0*trial.7
             quad =~ 9*trial.1 + 4*trial.2 + 1*trial.3 + 0*trial.4 + 
                     0*trial.5 + 0*trial.6 + 0*trial.7        
             slp2 =~ 0*trial.1 + 0*trial.2 + 0*trial.3 + 0*trial.4 + 
                     1*trial.5 + 2*trial.6 + 3*trial.7
             
"

quad.rate.fit <- growth(quad.rate,
                       data = trials,
                       estimator = "ML",
                       missing = "FIML")

summary(quad.rate.fit, fit.measures = FALSE, estimates = TRUE,
        standardize = TRUE, rsquare = FALSE)

ggplot(data.frame(id=quad.rate.fit@Data@case.idx[[1]], 
                           lavPredict(quad.rate.fit,type="ov")) %>% 
                  pivot_longer(cols = starts_with("trial"), 
                               names_to = c(".value", "num"), 
                               names_pattern = "(trial).(.)") %>%
                  dplyr::mutate(trial = as.numeric(trial)), 
                aes(x = num, 
                    y = trial, 
                    group = id, 
                    color = factor(id))) +
  geom_line() + 
  labs(title = "Inverse LCM Trajectories",
       x = "Age",
       y = "Predicted Externalizing Behavior") +
  theme(legend.position = "none") 

```

```{r piecewise 04, message = FALSE, warning = FALSE, error = FALSE}
adversity.long <- adversity %>% 
  pivot_longer(cols = starts_with("fmin"), 
               names_to = c(".value", "age"), 
               names_pattern = "(fmin)(.+)") %>%
  mutate(age = as.numeric(age),
         slp1 = ifelse(age > 7, 0, age - 7),
         slp2 = ifelse(age < 7, 0, age - 7),
         quad = ifelse(age > 7, 0, (age - 7)^2))

two.rate.mlm <- lmer(fmin ~ 1 + slp1 + slp2 + (1 + slp1 + slp2 | id),
                     na.action = na.omit,
                     REML = TRUE,
                     data = adversity.long,
                     control = lmerControl(optimizer = "bobyqa",
                                           optCtrl = list(maxfun = 2e5)))
summary(two.rate.mlm)
```

```{r piecewise 05, message = FALSE, warning = FALSE, error = FALSE}
two.rate.lcsm <- "
            # Define Phantom Variables (p = phantom)
            pfmin4 =~ 1*fmin4; fmin4 ~ 0; fmin4 ~~ fmin4; pfmin4 ~~ 0*fmin4
            pfmin5 =~ 1*fmin5; fmin5 ~ 0; fmin5 ~~ fmin5; pfmin5 ~~ 0*fmin5
            pfmin6 =~ 1*fmin6; fmin6 ~ 0; fmin6 ~~ fmin6; pfmin6 ~~ 0*fmin6
            pfmin7 =~ 1*fmin7; fmin7 ~ 0; fmin7 ~~ fmin7; pfmin7 ~~ 0*fmin7
            pfmin8 =~ 1*fmin8; fmin8 ~ 0; fmin8 ~~ fmin8; pfmin8 ~~ 0*fmin8
            pfmin9 =~ 1*fmin9; fmin9 ~ 0; fmin9 ~~ fmin9; pfmin9 ~~ 0*fmin9
            pfmin10 =~ 1*fmin10; fmin10 ~ 0; fmin10 ~~ fmin10; pfmin10 ~~ 0*fmin10
            pfmin11 =~ 1*fmin11; fmin11 ~ 0; fmin11 ~~ fmin11; pfmin11 ~~ 0*fmin11
            
            # Regressions Between Adjacent Observations
            pfmin4 ~ 1*pfmin5  # temporal order reversed before intercept
            pfmin5 ~ 1*pfmin6
            pfmin6 ~ 1*pfmin7
            pfmin8 ~ 1*pfmin7  # intercept time point appears twice
            pfmin9 ~ 1*pfmin8
            pfmin10 ~ 1*pfmin9
            pfmin11 ~ 1*pfmin10
            
            # Define Change Latent Variables (delta)
            # loadings prior to the intercept are negative
            delta21 =~ -1*pfmin4;  delta21 ~~ 0*delta21 
            delta32 =~ -1*pfmin5;  delta32 ~~ 0*delta32
            delta43 =~ -1*pfmin6;  delta43 ~~ 0*delta43
            
            # loadings after the intercept are as usual
            delta54 =~ 1*pfmin8;  delta54 ~~ 0*delta54
            delta65 =~ 1*pfmin9;  delta65 ~~ 0*delta65
            delta76 =~ 1*pfmin10;  delta76 ~~ 0*delta76
            delta87 =~ 1*pfmin11;  delta87 ~~ 0*delta87
            
            # Define Intercept and Slope
            int  =~ 1*pfmin7
            slp1 =~ 1*delta21 + 1*delta32 + 1*delta43
            slp2 =~ 1*delta54 + 1*delta65 + 1*delta76 + 1*delta87
        
            int ~ 1; slp1 ~ 1; slp2 ~ 1
            
            slp1 ~~ slp1
            slp2 ~~ slp2
            int ~~ slp1 + slp2
            slp1 ~~ slp2
"
two.rate.lcsm.fit <- sem(two.rate.lcsm,
                         data = adversity, 
                         estimator = "ML",
                         missing = "FIML")

summary(two.rate.lcsm.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = TRUE, rsquare = FALSE)
```

```{r piecewise 06, message = FALSE, warning = FALSE, error = FALSE}
two.rate.prop.lcsm <- "
            # Define Phantom Variables (p = phantom)
            pfmin4 =~ 1*fmin4; fmin4 ~ 0; fmin4 ~~ fmin4; pfmin4 ~~ 0*fmin4
            pfmin5 =~ 1*fmin5; fmin5 ~ 0; fmin5 ~~ fmin5; pfmin5 ~~ 0*fmin5
            pfmin6 =~ 1*fmin6; fmin6 ~ 0; fmin6 ~~ fmin6; pfmin6 ~~ 0*fmin6
            pfmin7 =~ 1*fmin7; fmin7 ~ 0; fmin7 ~~ fmin7; pfmin7 ~~ 0*fmin7
            pfmin8 =~ 1*fmin8; fmin8 ~ 0; fmin8 ~~ fmin8; pfmin8 ~~ 0*fmin8
            pfmin9 =~ 1*fmin9; fmin9 ~ 0; fmin9 ~~ fmin9; pfmin9 ~~ 0*fmin9
            pfmin10 =~ 1*fmin10; fmin10 ~ 0; fmin10 ~~ fmin10; pfmin10 ~~ 0*fmin10
            pfmin11 =~ 1*fmin11; fmin11 ~ 0; fmin11 ~~ fmin11; pfmin11 ~~ 0*fmin11
            
            # Regressions Between Adjacent Observations
            pfmin4 ~ 1*pfmin5  # temporal order reversed before intercept
            pfmin5 ~ 1*pfmin6
            pfmin6 ~ 1*pfmin7
            pfmin8 ~ 1*pfmin7  # intercept time point appears twice
            pfmin9 ~ 1*pfmin8
            pfmin10 ~ 1*pfmin9
            pfmin11 ~ 1*pfmin10
            
            # Define Change Latent Variables (delta)
            # loadings prior to the intercept are negative
            delta21 =~ -1*pfmin4;  delta21 ~~ 0*delta21 
            delta32 =~ -1*pfmin5;  delta32 ~~ 0*delta32
            delta43 =~ -1*pfmin6;  delta43 ~~ 0*delta43
            
            # loadings after the intercept are as usual
            delta54 =~ 1*pfmin8;  delta54 ~~ 0*delta54
            delta65 =~ 1*pfmin9;  delta65 ~~ 0*delta65
            delta76 =~ 1*pfmin10;  delta76 ~~ 0*delta76
            delta87 =~ 1*pfmin11;  delta87 ~~ 0*delta87
            
            # Define Proportional Change Regressions (beta = equality constraint)
            # Nonrecursive Proportional Paths
            delta21 ~ beta*pfmin4
            delta32 ~ beta*pfmin5
            delta43 ~ beta*pfmin6
            
            # Standard Proportional Paths
            delta54 ~ beta*pfmin7
            delta65 ~ beta*pfmin8
            delta76 ~ beta*pfmin9
            delta87 ~ beta*pfmin10
            
            # Define Intercept and Slope
            int  =~ 1*pfmin7
            slp1 =~ 1*delta21 + 1*delta32 + 1*delta43
            slp2 =~ 1*delta54 + 1*delta65 + 1*delta76 + 1*delta87
        
            int ~ 1; slp1 ~ 1; slp2 ~ 1
            
            slp1 ~~ slp1
            slp2 ~~ slp2
            int ~~ slp1 + slp2
            slp1 ~~ slp2
"
two.rate.prop.lcsm.fit <- sem(two.rate.prop.lcsm,
                              data = adversity, 
                              estimator = "ML",
                              missing = "FIML")

summary(two.rate.prop.lcsm.fit, fit.measures = FALSE, estimates = TRUE, 
        standardize = TRUE, rsquare = FALSE)
```
## Nonlinear Trajectories
*MLM: show negatively accelerated exponential*
*GAMM: full GAMM with b-splines*
*LCM: Show 2 free-loading forms*
*LCSM: dual-change model*


## Additional Considerations

### Fixed and Random Effects
*show in accelerated (braintime) data that we can estimate a quadratic overall trend even though no one has 4 observations*

### Generalizability
*Code for cross-validiation?*

```{r, cleanup shape, message = FALSE, warning = FALSE, error = FALSE, echo = FALSE}
rm(list = ls(all.names = FALSE))
```